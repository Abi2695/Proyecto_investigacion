# Proyecto_investigacion
El repositorio es la aplicaci√≥n del modelo random forest para predecir paras de producci√≥n.
# üåø Random Forest para anticipar **PARO MAQUINA** (l√≠neas 6 y 7)

Este proyecto/notebook (`Random_Forest_AF.ipynb`) entrena y eval√∫a modelos **Random Forest** para **predecir si ocurrir√° un paro de m√°quina** (`PARO MAQUINA` ‚àà {0,1}) a partir de variables operacionales de producci√≥n.

Incluye:

- Carga de datos desde un archivo Excel.
- Limpieza/transformaci√≥n de variables de fecha y hora (con tratamiento robusto de formatos de Excel).
- An√°lisis descriptivo y exploratorio (EDA).
- Entrenamiento de Random Forest **sin** y **con** balanceo de datos (SMOTE).
- Ajuste de umbral de decisi√≥n usando *precision-recall curve*.
- Guardado del pipeline final (preprocesamiento + SMOTE + modelo) y del umbral √≥ptimo.

---

## 1) üß∞ Requisitos

### 1.1. Software

- Python **3.9+** (recomendado 3.10 o 3.11).
- Jupyter Notebook / JupyterLab **o** Google Colab.

### 1.2. Librer√≠as

El notebook utiliza principalmente:

- `pandas`, `numpy`
- `matplotlib`, `seaborn`
- `scikit-learn`
- `imbalanced-learn` (para SMOTE)
- `joblib`

Instalaci√≥n recomendada (local):

```bash
pip install -U pandas numpy matplotlib seaborn scikit-learn imbalanced-learn joblib openpyxl
```

> Nota: `openpyxl` es el motor usual para leer `.xlsx` con `pandas.read_excel()`.

### 1.3. Archivo de datos

El notebook espera un Excel con columnas (al menos) como las siguientes:

- `FECHA`
- `OF`, `LINEA`, `TURNO`
- `CODIGO`, `PRODUCTO`, `SUPERVISOR`
- `INICIO_PROCESO`, `FIN_PROCESO`
- `TIEMPO ALM/MER`
- `DURACION (HORAS)` (se compara contra duraci√≥n calculada)
- `UNIDADES PRODUCIDAS`, `PRODUCCION X HORA`
- `PARO MAQUINA` (target)

En esta conversaci√≥n, el archivo se encuentra en:

- `Produccion linea 6 y 7 mayo a nov 2025.xlsx`

---

## 2) ‚ñ∂Ô∏è C√≥mo ejecutar (dos opciones)

### Opci√≥n A ‚Äî Google Colab (igual que el notebook)

1. Abrir `Random_Forest_AF.ipynb` en Colab.
2. Montar Drive:

```python
from google.colab import drive
drive.mount('/content/drive')
```

3. Definir `ruta` apuntando al Excel en Drive, por ejemplo:

```python
ruta = "/content/drive/MyDrive/Tesis Maestria UDLA/Base de datos/Produccion linea 6 y 7 mayo a nov 2025.xlsx"
df = pd.read_excel(ruta)
```

4. Ejecutar todas las celdas en orden.

**Archivos generados (Drive):**

- `/content/drive/MyDrive/Tesis Maestria UDLA/modelo_final_rf_smote.joblib`
- `/content/drive/MyDrive/Tesis Maestria UDLA/umbral_optimo.txt`

### Opci√≥n B ‚Äî Ejecuci√≥n local (Jupyter)

1. Clonar/copiar el notebook y el Excel a una carpeta.
2. Crear entorno (opcional pero recomendado):

```bash
python -m venv .venv
# Windows:
.\.venv\Scripts\activate
# macOS/Linux:
source .venv/bin/activate
```

3. Instalar dependencias:

```bash
pip install -U pandas numpy matplotlib seaborn scikit-learn imbalanced-learn joblib openpyxl
```

4. En el notebook, **reemplazar** la parte de Google Drive por una ruta local, por ejemplo:

```python
ruta = "./Produccion linea 6 y 7 mayo a nov 2025.xlsx"
df = pd.read_excel(ruta)
```

5. Cambiar las rutas donde se guardan modelo/umbral (si no usas Drive), por ejemplo:

```python
modelfinal_filename = "./modelo_final_rf_smote.joblib"
umbral_filename = "./umbral_optimo.txt"
```

---

## 3) üîÅ Reproducibilidad (importante)

El notebook fija `random_state=42` en:

- `RandomForestClassifier(..., random_state=42)`
- `SMOTE(random_state=42)`

y usa partici√≥n **temporal** (por fecha) en lugar de un split aleatorio:

- 80% m√°s antiguo ‚Üí `train`
- 20% m√°s reciente ‚Üí `test`

Para que los resultados sean lo m√°s reproducibles posible:

- Mant√©n las mismas versiones de librer√≠as.
- No cambies el orden de las filas antes del `sort_values("FECHA")`.
- Ejecuta las celdas en orden.

### Guardar versiones (recomendado)

En local, puedes generar un archivo de versiones:

```bash
pip freeze > requirements.txt
```

Y luego reproducir el entorno con:

```bash
pip install -r requirements.txt
```

---

## 4) üß≠ Paso a paso del notebook

A continuaci√≥n se describe lo que hace cada bloque (equivale a las secciones del notebook).

### 4.1 Importaci√≥n de librer√≠as

Se importan `pandas`, `numpy`, `matplotlib` y, m√°s adelante, `seaborn`, `sklearn`, `imblearn` y `joblib`.

### 4.2 Carga del archivo Excel

- Montaje de Drive (si aplica).
- Lectura con `pd.read_excel(ruta)`.
- Normalizaci√≥n de nombres de columnas:
  - `strip()`
  - reemplazo de espacios m√∫ltiples por uno.

Se imprimen:

- Dimensi√≥n del dataset (`df.shape`)
- Lista de columnas
- Tipos de datos
- Primeras filas

### 4.3 Limpieza y transformaci√≥n de fechas/horas

1) **`FECHA`** se convierte a `datetime`:

```python
df2["FECHA"] = pd.to_datetime(df2["FECHA"], errors="coerce")
```

2) Se crea una funci√≥n robusta `time_to_seconds()` para convertir `INICIO_PROCESO` y `FIN_PROCESO` a **segundos desde medianoche**, soportando:

- Horas guardadas como n√∫mero en Excel (fracci√≥n del d√≠a).
- Texto o datetime (por ejemplo `1900-01-01 00:05:00`).
- Texto tipo `HH:MM:SS` usando `to_timedelta`.

3) Se construyen timestamps completos:

- `inicio_dt = FECHA + inicio_sec`
- `fin_dt = FECHA + fin_sec`

4) Se corrige el caso en que la producci√≥n **cruza medianoche** (cuando `fin_sec < inicio_sec`):

- `fin_dt = fin_dt + 1 d√≠a`

5) Se calcula una duraci√≥n confiable:

- `duracion_calc_horas = (fin_dt - inicio_dt)` en horas.

Adem√°s, el notebook muestra una muestra comparativa entre `DURACION (HORAS)` y `duracion_calc_horas`.

### 4.4 Verificaci√≥n del balance de clases

Se revisa la distribuci√≥n de `PARO MAQUINA`:

- Conteo y porcentaje.
- Tasa de paro por `LINEA` y por `TURNO`.

Esto ayuda a justificar (o no) el uso de t√©cnicas de balanceo como **SMOTE**.

### 4.5 An√°lisis descriptivo y exploratorio (EDA)

Se crean variables adicionales para explorar patrones:

- `dia_semana` (0=lunes, ..., 6=domingo)
- `mes`
- `inicio_hora`, `inicio_min`
- `alm_mer_min`: `TIEMPO ALM/MER` convertido a minutos
- `duracion_horas_final` (equivalente a `duracion_calc_horas`)

Luego se realiza:

- `describe()` sobre variables num√©ricas + conteo de valores faltantes.
- Gr√°fico de distribuci√≥n de `PARO MAQUINA`.
- Boxplots de variables num√©ricas vs. `PARO MAQUINA` (si existen en el dataset).
- Matriz de correlaci√≥n **Spearman** para variables num√©ricas.
- Tendencia temporal: tasa de paro mensual.

### 4.6 Preparaci√≥n de datos para el modelo

Se construye un dataset `df_model` con **features permitidas** para anticipar el paro (evita variables que pudieran causar fuga de informaci√≥n).

Features usadas:

- `OF`, `LINEA`, `TURNO`
- `CODIGO`, `PRODUCTO`, `SUPERVISOR`
- `inicio_hora`, `inicio_min`
- `dia_semana`, `mes`
- `alm_mer_min`

`FECHA` se conserva solo para **ordenar temporalmente**, pero se excluye del entrenamiento.

### 4.7 Split temporal Train/Test

- Se ordena por `FECHA`.
- Se define un corte al 80% del dataset:

```python
cut = int(len(df_model) * 0.8)
train_df = df_model.iloc[:cut]
test_df  = df_model.iloc[cut:]
```

Se reporta el rango de fechas en train/test y la distribuci√≥n de la clase en cada partici√≥n.

### 4.8 Pipeline de preprocesamiento

Se separan columnas por tipo:

- Num√©ricas ‚Üí imputaci√≥n por mediana (`SimpleImputer(strategy="median")`).
- Categ√≥ricas ‚Üí imputaci√≥n por moda + one-hot encoding (`OneHotEncoder(handle_unknown="ignore")`).

Esto se implementa con `ColumnTransformer` y se integra a un `Pipeline`.

### 4.9 Random Forest sin SMOTE (3 corridas)

Se eval√∫an 3 configuraciones (con `class_weight="balanced"`):

1. `RF_noSMOTE_1_baseline`
2. `RF_noSMOTE_2_regularizado`
3. `RF_noSMOTE_3_prof_moderada`

M√©tricas que imprime el notebook:

- ROC-AUC
- Matriz de confusi√≥n (umbral 0.5)
- `classification_report` (precision/recall/f1)

Adem√°s, se construye un resumen (DataFrame) ordenado por ROC-AUC.

### 4.10 Random Forest con SMOTE (3 corridas)

Se repiten 3 configuraciones, pero ahora con un pipeline `imblearn`:

`preprocess -> SMOTE -> model`

Experimentos:

1. `RF_SMOTE_1_baseline`
2. `RF_SMOTE_2_regularizado`
3. `RF_SMOTE_3_prof_moderada`

Se imprimen las mismas m√©tricas y un resumen por ROC-AUC.

### 4.11 Ajuste del umbral de decisi√≥n

Con el **mejor modelo SMOTE** (en el notebook: `RF_SMOTE_1_baseline`), se calculan:

- `precision_recall_curve(y_test, proba)`
- F1 para cada umbral
- Umbral que maximiza F1

Luego se vuelve a evaluar el modelo con ese umbral:

- Matriz de confusi√≥n
- Reporte de clasificaci√≥n

> Importante: Si tu objetivo principal es **maximizar recall** (minimizar falsos negativos), puedes modificar el criterio: por ejemplo elegir el umbral que logre un recall m√≠nimo deseado y luego maximizar precisi√≥n o F1 dentro de esa regi√≥n.

### 4.12 Comparaci√≥n de m√©tricas: umbral 0.5 vs umbral √≥ptimo

El notebook incluye una comparaci√≥n gr√°fica (barras) entre m√©tricas para clase 1:

- Precision
- Recall
- F1

**Nota:** en esa celda, los valores est√°n ingresados manualmente (hard-coded). Si cambias datos/semillas/modelos, actualiza esos n√∫meros para que reflejen tu ejecuci√≥n real.

### 4.13 Importancia de variables

Se carga el pipeline guardado y se extraen:

- El `model.feature_importances_`
- Los nombres de features despu√©s del preprocesamiento (num√©ricas + one-hot de categ√≥ricas)

Se grafica la importancia para interpretar qu√© variables (y qu√© categor√≠as) aportan m√°s al modelo.

### 4.14 Guardado del modelo final y umbral

Se guardan:

- Pipeline completo (`joblib.dump`):
  - `modelo_final_rf_smote.joblib`
- Umbral √≥ptimo a un `.txt`:
  - `umbral_optimo.txt`

En Colab, se guardan en Drive bajo `.../Tesis Maestria UDLA/`.

### 4.15 C√≥mo usar el modelo guardado para predecir

Pasos:

1. Cargar el pipeline con `joblib.load()`.
2. Leer el umbral desde `umbral_optimo.txt`.
3. Preparar un DataFrame nuevo `X_new` con las **mismas columnas de features** que se usaron en entrenamiento.
4. Obtener probabilidades con:

```python
proba = loaded_pipeline.predict_proba(X_new)[:, 1]
```

5. Convertir a predicci√≥n final usando el umbral:

```python
pred = (proba >= loaded_optimal_threshold).astype(int)
```

---

## 5) Estructura de artefactos (salidas)

- `modelo_final_rf_smote.joblib`: pipeline completo entrenado (preprocesamiento + SMOTE + RandomForest).
- `umbral_optimo.txt`: umbral num√©rico (float) calculado para maximizar F1 (seg√∫n el notebook).

---

## 6)üí° Notas y recomendaciones

- **Split temporal**: es apropiado cuando se quiere simular predicci√≥n hacia el futuro y evitar fuga de informaci√≥n.
- **Valores faltantes**: se imputan autom√°ticamente (mediana para num√©ricas, moda para categ√≥ricas).
- **Celdas con valores manuales**: la comparaci√≥n de m√©tricas por umbral (celda de barras) usa n√∫meros hard-coded. Para reproducibilidad estricta, conviene calcularlos directamente desde `y_test`, `proba_best_smote` y el `optimal_threshold`.

---

## 7) Referencia r√°pida (comandos)

Instalar dependencias:

```bash
pip install -U pandas numpy matplotlib seaborn scikit-learn imbalanced-learn joblib openpyxl
```

Abrir notebook local:

```bash
jupyter lab
# o
jupyter notebook
```
